{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy\n",
    "import textacy.vsm\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') ## install it by python -m spacy download en (run as administrator)\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('./incoPat_225509/*.xls')\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    try:\n",
    "        dfs.append(pd.read_excel(f))\n",
    "    except Exception as e:\n",
    "        print(\"{0}: {1}\".format(f, e))\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df['标题(英)'].apply(lambda x: nlp(x.lower())).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf * log((n_docs + 1) / (df + 1)) + 1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = textacy.Corpus(nlp, data =docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = textacy.vsm.Vectorizer(\n",
    "    tf_type=\"linear\", apply_idf=True, idf_type=\"smooth\", apply_dl=False, min_df=0.0001, max_df=0.01) \n",
    "doc_term_matrix = vectorizer.fit_transform(\n",
    "    (doc._.to_terms_list(ngrams=2,normalize='lemma', entities=False, as_strings=True, filter_stops=True, filter_nums=True, filter_punct=True, drop_determiners=True) for doc in corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectorizer.terms_list)\n",
    "# print(doc_term_matrix.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dict = {}\n",
    "np_matrix = doc_term_matrix.todense()\n",
    "for term in vectorizer.terms_list:\n",
    "    tfidf_dict[term] = np.max(np_matrix[:,vectorizer.vocabulary_terms[term]])\n",
    "sorted_x = sorted(tfidf_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_dict = collections.OrderedDict(sorted_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_dict = collections.OrderedDict()\n",
    "re_ = r\"^A |^THE |^THEIR |^ITS |^THIS |^AN |^SUCH A |^AS |^TO |^AND |^-PRON- |AT LEAST|USE THEREOF|^OR | THEREOF| THEREFOR| THERETO| THEREFROM| THEREBY\"\n",
    "for k, v in sorted_dict.items():\n",
    "    k = re.sub(re_, \"\", k.upper())\n",
    "    if k.count(' ') < 5 and k.count(' ') > 1:\n",
    "        filter_dict[k] = v\n",
    "filter_df = pd.DataFrame.from_dict(filter_dict, orient='index', columns=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_df.head(1000).to_excel('./out/kws_tfidf.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyterProject)",
   "language": "python",
   "name": "pycharm-29c72c67"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
