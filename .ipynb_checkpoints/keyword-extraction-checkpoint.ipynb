{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import en_core_web_sm\n",
    "from collections import Counter\n",
    "\n",
    "import re\n",
    "import swifter\n",
    "# from textblob import TextBlob\n",
    "\n",
    "import sys, os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# import import_ipynb\n",
    "# import Ipynb_importer\n",
    "\n",
    "# %run translation_api.ipynb\n",
    "# %run Ipynb_importer\n",
    "\n",
    "# with __import__('importnb').Notebook():\n",
    "#     import translation_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('./incoPat_225509/*.xls')\n",
    "\n",
    "dfs = []\n",
    "for f in files:\n",
    "    try:\n",
    "        dfs.append(pd.read_excel(f))\n",
    "    except Exception as e:\n",
    "        print(\"{0}: {1}\".format(f, e))\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['序号', '标题(中)', '摘要(中)', '公开(公告)号', ' 最早公开（公告）日', '申请号', '最早申请日', '同族国家',\n",
       "       '族成员个数', '扩展同族ID', '链接到incoPat', '标题(英)', '摘要(英)', '合享价值度', '优先权信息',\n",
       "       '专利有效性', 'IPC主分类', 'IPC', '国民经济分类', '申请人', '标准申请人', '申请人国别代码',\n",
       "       '申请人省市代码', '发明人', '被引证专利', '引证专利', '引证申请人', '被引证申请人', '被引证次数', '转让人',\n",
       "       '受让人', '转让执行日', '当前被许可人'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "re_ = r\"^A |^THE |^THEIR |^ITS |^THIS |^AN |^SUCH A |^AS |^TO |^AND |^-PRON- |AT LEAST|USE THEREOF|^OR | THEREOF| THEREFOR| THERETO| THEREFROM| THEREBY\"\n",
    "\n",
    "# a = 'A METHOD OF MANUFACTURING EPITAXIAL SILICON WAFERS'\n",
    "# b = 'APPARATUS FOR BURNING IN ELECTRONIC COMPONENTS'\n",
    "# c = 'stem cells'\n",
    "# doc = nlp(c)\n",
    "# rt = lambda x:[re.sub(re_, pd.NA,np.lemma_.upper())\n",
    "#                if ' ' in re.sub(re_, pd.NA,np.lemma_.upper()) else '' for np in doc.noun_chunks]\n",
    "# rt(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304bdfe43d3145028da5074745b6128a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Pandas Apply'), FloatProgress(value=0.0, max=225509.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df['parsed_title'] = df['标题(英)'].swifter.apply(lambda x: [re.sub(re_, \"\", y.lemma_.upper()) if ' ' in re.sub(re_, \"\", y.lemma_.upper()) else '' for y in nlp(x).noun_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.parsed_title = df.parsed_title.apply(lambda x: list(filter(None,x)))\n",
    "df.to_csv('./tmp/parsed_title.csv', columns=[\"parsed_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kws = df.parsed_title.explode().replace(\"\",pd.NA).dropna().str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict(Counter(kws))\n",
    "fq = pd.DataFrame.from_dict(d, orient='index', columns=['frequency']).sort_values(by='frequency', ascending=False)\n",
    "fq = fq[(fq.frequency > 23) & (fq.frequency < 10000)]\n",
    "fq.to_excel('./out/kw_dp_term_frequency.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CENTRIFUGALLY CAST</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STABLE EMULSION COMPOSITION</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEMICONDUCTOR PORCELAIN COMPOSITION</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DOUBLE - SIDED FLEXIBLE PRINT BOARD</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EXCITATORY AMINO ACID RECEPTOR ANTAGONIST</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RESIN COMPOSITION</th>\n",
       "      <td>1170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FUEL CELL</th>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRODUCTION METHOD</th>\n",
       "      <td>1748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MANUFACTURING METHOD</th>\n",
       "      <td>2593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEMICONDUCTOR DEVICE</th>\n",
       "      <td>2655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35840 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           frequency\n",
       "CENTRIFUGALLY CAST                                 2\n",
       "STABLE EMULSION COMPOSITION                        2\n",
       "SEMICONDUCTOR PORCELAIN COMPOSITION                2\n",
       "DOUBLE - SIDED FLEXIBLE PRINT BOARD                2\n",
       "EXCITATORY AMINO ACID RECEPTOR ANTAGONIST          2\n",
       "...                                              ...\n",
       "RESIN COMPOSITION                               1170\n",
       "FUEL CELL                                       1597\n",
       "PRODUCTION METHOD                               1748\n",
       "MANUFACTURING METHOD                            2593\n",
       "SEMICONDUCTOR DEVICE                            2655\n",
       "\n",
       "[35840 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fq.sort_values('frequency')\n",
    "# TODO fuel cells = fuel cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks you need to narrow down more than just keywords/key phrases and find the subject and object per sentence. For subject/object recognition, I recommend the Stanford Parser or the Google Language API, where you send a string and get a dependency tree response.\n",
    "\n",
    "You can test the Google API first to see if it works well with your corpus: https://cloud.google.com/natural-language/\n",
    "\n",
    "The outcome here is a subject predicate object (SPO) triplet, where your predicate describes the relationship. You'll need to traverse the dependency graph and write a script to parse out the triplet.\n",
    "\n",
    "Other Packages: I use NLTK, Spacy, and Textblob frequently. If the corpus is simple, generic, and straightforward, Spacy and Textblob work well OOTB. If the corpus is highly customized, domain-specific, messy (incorrect spelling or grammar), etc. I'll use NLTK and spend more time customizing my NLP text processing pipeline with scrubbing, lemmatizing, etc. You may want to add your own custom dictionary of technology related keywords and keyphrases so that your parser can catch these if you decide to go with one of these packages.\n",
    "\n",
    "NLTK Tutorial: http://www.nltk.org/book/\n",
    "\n",
    "Spacy Quickstart: https://spacy.io/usage/\n",
    "\n",
    "Textblob Quickstart: http://textblob.readthedocs.io/en/dev/quickstart.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (jupyterProject)",
   "language": "python",
   "name": "pycharm-29c72c67"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
